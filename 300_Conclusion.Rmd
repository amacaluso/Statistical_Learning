<h2><a name="3_Conclusion" style="pointer-events: none; cursor: default;" ><font color="black">3. Conclusion</font></a></h2>

n this report, we have conducted a classification analysis and we have compared many different approaches to this end, in a view typical of the data science field: selecting the model that guarantees the best out-of-samples predictions.
Nonetheless, theoretical insights on the statistical properties of several models have also been treated.

In summary, the comparison takes into account the adoption of:

- linear classification models, i.e. *Linear Probability Model*, *Linear Discriminant Analysis*, *Logistic Regression* and the regularised versions through *Ridge*, *Lasso* and *Elastic-net* penalties;

- non-linear classification models, i.e. *Quadratic Discriminant Analysis*, *K-Nearest Neighbour* and *Generalised Additive Models*;

The table below shows the optimal threshold for classification according to each of the model explored, together with the relative test set accuracy, sensitivity, specificity and AUC.


```{r, fig.width=9.5, fig.height=4, echo=FALSE}

file = "results/MODELING/CLASSIFICATION/ROC_best.Rdata"
load( file )

df[, -c(1,2)] = round(df[,-c(1,2)], 4)

datatable(df, 
          options = list(pageLength = 9), 
          rownames = F, 
          class = "display")
```

<br></br>
<br></br>

Looking at the accuracy metric, the GAM is the one performing better (0.7735), followed by KNN (0.7512) and all of the linear models. Notice that, excluding the regularised approaches, QDA is the worst notwithstanding the use of a more complex and flexible separation surface.
On the other hand, it is interesting to notice how the LPM is placed on the last step of the podium (0.7488) despite its simplicity and the theoretical inadequacy to represent a probability through an output not bounded in $[0,1]$. 
In general, this can be explained observing that, in a classification problem, the critical task is not to predict precisely the probability of each category but rather to separate the classes properly.

Considering the AUC instead, the performances of the various models remain unchanged in terms of comparison. The only exception is KNN that falls from the second to the last position, thus shifting up all the intermediate models.

```{r, fig.width=9.5, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/roc_curve_all.Rdata"
load( file )
plot
```

<br></br>
<br></br>

In conclusion, the GAM is the best model both in terms of accuracy and AUC, strictly followed by KNN and LMP.

However, the performances of the various models are quite similar, as we can see from the ROC curves above, which are all very close.
That being considered, one may decide to prefer a more parsimonious model in order to gain in simplicity and interpretability at the expense of a minimal loss in predictive performance.

<br></br>
<br></br>
