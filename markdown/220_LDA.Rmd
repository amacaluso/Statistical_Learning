<h2><a name="22_LDA" style="pointer-events: none; cursor: default;" ><font color="black"> 2.2. Linear Discriminant Analysis</font></a></h2>

The linear discriminant analysis (LDA) consists of an alternative solution for building a discriminant function. Indeed, rewriting $P(Y=g|X)$ as $\frac{f_g(x)\pi_g}{\sum_{l=1}^{G}f_l(x)\pi_l}$ it is possible to estimate the probability of each class based on its density function (and its prior probability). Starting from this formulation, the linear discriminant analysis assumes that the $f_g(x)$ are $p+1$-dimensional gaussian distribution with common covariance matrix.

Once the $f_g(x)$ are estimated based on sample data, it is then possible to show that the discriminant function associated to each category turns out to be:
$$\delta_g(x)=x^{T}\Sigma^{-1}\mu_g -\frac{1}{2}\mu_g^T\Sigma^{-1}\mu_g+\log{\pi_g}$$
Finally, in order to classify observations into one of the classes we simply pick $g_*=argmax_g\delta_g(x)$.

Passing to the application, the model reaches an accuracy of 0.7438 and an AUC of 0.8095.


```{r, fig.width=9.5, fig.height=4, echo=FALSE}
file = "../results/MODELING/CLASSIFICATION/lda_line_1_vs_0.Rdata"
load( file )
plot
```

<br></br>
<br></br>


Alternatively, the LDA approach can be rephrased as finding the linear combination of the regressors that better separates the categories of the target variable. In other words, we have to find  $a^Tx$ such that the groups are internally as homogeneous as possible and as distant as possible among each other. This means we want low variability within each class and high between variance:
$$\phi_*=argmax_{\textbf{a}}\frac{\textbf{a}^TB\textbf{a}}{\textbf{a}^TW\textbf{a}}$$
where $B$ is the between-groups covariance matrix and $W$ the within one.

Solving this optimisation problem we get:
$$(W^{-1}B-\phi I_{G \times G})\textbf{a}=0$$

This is a homogeneous system of linear equations, so it admits non-trivial solutions if and only if $(W^{-1}B-\phi I)=0$, i.e. if $\phi$ is an eigenvalue of $W^{-1}B$ and $a$ contains the correspondent eigenvector. Algebraically, this implies that we have at most $G-1$ solutions, precisely as many as the rank of $W^{-1}B$. As a consequence, there are no more than $G-1$ discriminant directions. Since $\phi$ is exactly the quantity we want to maximise, the best discriminant function is given by the eigenvector corresponding to the biggest eigenvalue of $W^{-1}B$. Namely, it is the surface that better separates one class from all the others.


In the case of two categories, we have a single non-zero eigenvalue and, hence, a single discriminant direction. To understand better the results of this approach, the following plot displays the observations with respect to the linear combination  $a^Tx$, also called canonic coordinate, with the linear decision boundary superimposed.

```{r, fig.width=9.5, fig.height=4, echo=FALSE}
file = "../results/MODELING/CLASSIFICATION/canonical_variable.Rdata"
load( file )

file = "../results/MODELING/CLASSIFICATION/canonical_variable2.Rdata"
load( file )

subplot( canonical_variable, canonical_variable2) %>% layout(  title = 'Canonical variable')

```

<br></br>
<br></br>

Notice that the two classes do not appear very separated. Probably, the quite decent performances of the model are because the dataset is unbalanced and that the model correctly classifies the most represented categories. i.e. observations of class 1.

Comparing the results of the two LDA formulations, we observe that the vector $a$ is unexpectedly different from the *Coefficients of linear discriminants* computed by the R routine.
However, this is merely a matter of parameterisation. In fact, R uses a normalisation that makes the within-covariance matrix spherical, i.e.:

$$ \textbf{a}^TW\textbf{a} = \Psi_{diag} $$

Therefore, starting from the second formulation, it is possible to get the same coefficients of the function *lda* post-multiplying the vector $\textbf{a}$ by the matrix $\Psi^{-\frac{1}{2}}$.

<br></br>
<br></br>
