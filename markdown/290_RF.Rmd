<h2><a name="230_RF" style="pointer-events: none; cursor: default;" ><font color="black"> 2.3. Random Forest</font></a></h2>

Gli alberi sono uno strumento molto semplice ma anche molto potente, tuttavia solitamente sono caratterizzati da una grande variabilità, i.e. un piccolo cambiamento nei dati può comportare una sequenza di split molto differente. Ciò è dovuto principalmente al fatto che un errore in uno dei primi split viene propagato gerarchicamente in tutta la struttura sottostante. Per ovviare a questo inconveniente è possibile usare metodi di *bagging* che consentono di creare una serie di alberi, per l'appunto una foresta, e ottenere una stima con ridotta variabilità mediando tra questi.
Il bagging, infatti, consiste nel suddividere il campione iniziale in $B$ sottocampioni bootstrap, anche detti *bag*, di dimensioni $n_b < n$ e nello stimare un modello diverso in ciascuno di essi, validandolo poi sulle $n-n_b$ osservazioni rimanenti, cosiddette *Out Of Bag*.
Il modello predittivo globale viene costruito, infine, effettuando una media tra molti modelli (in questo caso in forma di alberi) in modo da ridurne la varianza. 

In quest'ottica, una Random Forest è ottenuta allenando un gran numero di alberi che vengono fatti crescere con criteri molto blandi così da avere una distorsione auspicabilmente limitata malgrado una varianza elevata, che verrà però ridotta attraverso l'operazione di *averaging*:

$$\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^B\hat{f}^{*b}(x) $$

Inoltre, per evitare che gli alberi adottati siano tra loro indipendenti (caratteristica che limita i benefici nella riduzione della variabilità delle stime dell'approccio bagging), nell'addestramento della RF viene applicato un campionamento bootstrap dei regressori. In questo modo solamente un campione casuale di *m < p* covariate partecipa alla crescita di ogni albero e ciò permette di ridurre ulteriormente la variabilità del modello finale.

Stante questa descrizione, si deduce che le Random Forest fanno ricorso a due parametri di aggiustamento che corrispondono al numero $M$ di regressori e il numero $B$ di alberi da costruire all'interno della foresta. 
Nel caso in esame, il tuning di questi due parametri è stato condotto sulla base di una grid-search esaustiva in cui $M$ variava da 1 a 500 e $B$ da 1 a 12 (massimo numero di regressori). Così facendo, le foreste casuali confrontate spaziavano da una molto semplice (un solo albero, $B=1$, con una sola variabile, $M=1$) ad una molto complessa (cinquecento alberi, $B=500$, e tutte le variabili, $M=12$).


Per quanto riguarda le variaibli considerate, essendo le random forest basate sulla creazione di molti alberi, non è possibile osservare gli split come accade nel singolo albero ma possiamo comunque valutare l'importanza delle variabili utilizzate.
In questo caso la variable importance dipende dalla riduzione in termini di errore derivante dall'aver seguito uno split in corrispondendenza di quella variabile, pesato su tuttu gli alberi.


```{r, fig.width=9.5, fig.height=4, echo=FALSE}
file = "../results/MODELING/CLASSIFICATION/RF_valid_err.Rdata"
load( file )
plot

```
<br></br>
<br></br>

I risultati di questa validazione sono presentati nella figura soprastante. Come si può notare, sfruttando le osservazioni Out Of Bag è stato possibile ottimizzare questi due parametri, scegliendo una combinazione riconducibile ad una foresta con $B=239$ e solo $M=4$ regressori.

Le performance così ottenute da questo modello sono molto elevate con un'accuracy di 0.8225 e un'AUC di 0.8935. Le random forest si presentano dunque come il miglior modello utilizzato, seppur caratterizzato da una minor interpretabilità rispetto ai precedenti e, in particolare, rispetto al singolo albero.

Infine, nel grafico seguente è riportata la variable importance, calcolata stavolta sfruttando le osservazioni OOB anzichè la cross-validation.

```{r, fig.width=9.5, fig.height=4, echo=FALSE}
file = "../results/MODELING/CLASSIFICATION/plot_importance_rf.Rdata"
load( file )
plot
```

Anche in questo caso la variabile *alcohol* è quella che assume una rilevanza maggiore. Tuttavia, stavolta è *volatile.acidity* a piazzarsi in seconda posizione, seguita a poca distanza da *density*.

<br></br>
<br></br>

<br></br>
<br></br>