##<a name="21_LPM" style="pointer-events: none; cursor: default;" ><font color="black"> 2.1. Linear Probability Model</font></a>

Il modello di regressione si inserisce nel contesto dei metodi di classificazione che si basano sulla definizione di una funzione descriminante, $\delta_k(x)$, dalla quale poi viene derivata l'appartenenza di una osservazione ad una particolare classe. In particolare, la soluzione proposta da questa tecnica è quella di scegliere come funzione discriminante $E[Y|X]$ che, vista la natura dicotomica del problema (i.e. una osservazione o appartiene o non appartiene ad una data modalità), equivale alla probabilità di appartenenza ad una generica classe g, $P(Y=g|X)$.

In generale, immaginando di avere un problema di classificazione multinomiale ($G$ modalità) con $p$ regressori, questo approccio consiste nell'adottare un modello di regressione lineare in cui la variabile risposta **Y** è una matrice di variabili indicatrici, ciascuna avente valore 1 se l'osservazione appartiene alla relativa modalità della variabile di output e 0 altrimenti.
Nel caso specifico di una classificazione binaria, possiamo tuttavia adattare un solo modello al dataset di training ed utilizzarlo per calcolare uno score che servirà poi per classificare ciascuna osservazione in base ad un dato threshold (da determinare). Si noti che lo score calcolato sarà per costruzione un numero reale e, quindi, malgrado lo si utilizzi per modellare la probabilità di appartenenza ad una classe - di qui il nome Linear Probability Model (LPM) -, non è possibile interpretarlo come una probabilità. Tuttavia, è possibile dimostrare che sommando gli score di una singola osservazione ottenuti dai modelli relativi a ciascuna modalità il risultato è 1. Infatti:

$$
\hat{Y}_{n \times G} = X_{n \times (p+1)} ({X}^{T} X)_{(p+1) \times (p+1)}^{-1} {X}_{(p+1) \times n}^{T}  Y_{n \times G}$$

Post-moltiplicando poi per un vettore colonna G-dimensionale formato da soli 1, possiamo sommare gli elementi di $\hat{Y}$:

$$\hat{Y}_{n \times G}  \textbf{1}_{G \times 1} = X_{n \times (p+1)}  ({X}^{T} X)_{(p+1) \times (p+1)}^{-1}  {X}_{(p+1) \times n}^{T}  Y_{n \times G}  \textbf{1}_{G \times 1}$$

A questo punto, pre-moltiplicando a destra per la matrice identità di ordine $n$ ottenuta come $(X {X}^{T})^{-1}(X {X}^{T})$, è facile verificare che l'equazione precedente si riduce semplicemente a:

$$(X {X}^{T})^{-1}(X {X}^{T})  X_{n \times (p+1)}  ({X}^{T} X)_{(p+1) \times (p+1)}^{-1}  {X}_{(p+1) \times n}^{T}  Y_{n \times G}  \textbf{1}_{G \times 1}$$

$$(X {X}^{T})^{-1}X \biggl[({X}^{T}  X_{n \times (p+1)})  ({X}^{T} X)_{(p+1) \times (p+1)}^{-1}\biggr]  {X}_{(p+1) \times n}^{T}  Y_{n \times G}  \textbf{1}_{G \times 1}$$

$$\biggl[(X {X}^{T})^{-1}X  {X}_{(p+1) \times n}^{T}\biggr]  Y_{n \times G}  \textbf{1}_{G \times 1}$$

$$\hat{Y}_{n \times G}  \textbf{1}_{G \times 1} = Y_{n \times G}  \textbf{1}_{G \times 1}$$

Per cui si ricava che la somma degli score predetti è 1 per ciascuna osservazioni dal momento che le funzioni indicatrici (RHS) vale $\hat{Y_1}+ ... + \hat{Y_G} = 1$ per costruzione.

Per cui, malgrado gli score non siano probabilità, sono accomunate ad esse da questa proprietà.

Passando all'applicazione, è facile verificare come lo score predetto dal modello non è in generale compreso tra 0 e 1, come si evince dal seguente grafico.

```{r, fig.width=9.5, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/lpm_probs.Rdata"
load( file )
plot
```

<br></br>
<br></br>

Dando uno sguardo all'output, possiamo notare come quasi tutte le variabili risultino significative per qualsiasi soglia $\alpha$ ragionevole. Le uniche eccezioni riscontrate sono *chlorides* e *citric.acid* con p-value rispettivamente di 0.37 e 0.08.

```{r, fig.width=9.5, fig.height=4, echo=FALSE}

file = "results/MODELING/CLASSIFICATION/lpm_summary.Rdata"
load( file )

datatable(df, 
          options = list(pageLength = 13), 
          rownames = F, 
          class = "display")
```

<br></br>
<br></br>

Se ci focalizziamo sulle stime dei coefficienti invece, è possibile notare come la variabile che sembra contare di più nella previsione è *density*. Tuttavia, l'alto valore di questo coefficiente (-36.12) sembra bilanciarsi sia in termini di stima che di variabilità con quello dell'intercetta. Questo comportamento è dovuto al fatto che la variabile sia molto più concentrata delle altre ed è anche correlata con l'output, per cui una minima variazione del suo valore è associata ad un grande cambiamento nel valore predetto. Infatti, eliminando questo effetto tramite standardizzazione, la stima per *density* sembra più in linea con gli ordini di grandezza degli altri coefficienti e le variabili che incidono maggiormente diventano *volatile.acidity*, *alcohol* e *type* (rispettivamente -0.15, 0.14 e - 0.13). A seguito della trasformazione non variano, invece, i risultati di significatività.

```{r, fig.width=9.5, fig.height=4, echo=FALSE}

file = "results/MODELING/CLASSIFICATION/lpm_summary_std.Rdata"
load( file )

datatable(df, 
          options = list(pageLength = 13), 
          rownames = F, 
          class = "display")
```

<br></br>
<br></br>

Una volta ottenuti i valori predetti, per poter classificare le osservazioni in una delle due classi è necessario poi fissare un taglio sulla previsione del modello che funga da separatore delle due classi. La scelta di questa soglia non è tuttavia così intuitiva come in altri modelli, proprio perchè l'outcome non è vincolato all'intervallo $[0,1]$. Per questa ragione, la scelta del threshold ottimale è stata fatta sulla base di una *grid-search* in modo da minimizzare l'errore di classificazione nel test set. In particolare, abbiamo provato una griglia di valori da 0 a 1 con gap di 0.01 ed è risultata ottimale la classificazione relativa ad una soglia di 0.54.

Una volta scelto il taglio ottimale con l'ausilio del test set, non è più possibile utilizzare quegli stessi dati per dare una misura della capacità di generalizzazione del modello. Infatti, la misura così calcolata esprimerebbe una sovrastima dell'accuratezza dal momento che il test set è stato già dato in pasto al modello per ottimizzare il threshold. 
Per evitare ciò, abbiamo ricavato una stima dell'abilità previsiva per Leave One Out cross-validation sfruttando il fatto che, nel caso di regressione lineare con funzione di perdita quadratica, è possibile calcolare questa misura stimando una sola volta il modello anzichè $n$:

$$ GCV = \frac{1}{n} \sum_{i=1}^{n} { \biggl[ \frac{ y_i - \hat{f}(x_i) }
{1- \frac{trace(S)}{n}} \biggr]^2 } $$

dove $trace(S)$ rappresenta il numero di gradi di libertà effettivi del modello (nel nostro caso $p+1=13$)

Si noti che, malgrado la misura di performance di interesse non sia una funzione di perdita quadratica, è facile verificare come in questo caso essa coincida proprio con l'errore di classificazione per via della natura dicotomica di $y_i$ e $\hat{f}(x_i)$.

In conclusione, l'accuratezza ottenuta attraverso la Generalised cross-validation è di 0.7402 (a fronte del 0.7488 calcolato nel test set che rappresenta, come previsto, una stima ottimistica). In termini di  Receiver Operating Characteristic (ROC) curve, invece, il modello presenta un Area Under Curve (AUC) di 0.8095. 


<br></br>
<br></br>

