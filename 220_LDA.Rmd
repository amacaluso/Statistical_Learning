##<a name="22_LDA" style="pointer-events: none; cursor: default;" ><font color="black"> 2.2. Linear Discriminant Analysis</font></a>

L'analisi discriminante lineare (LDA) consiste in un tentativo alternativo di costruire una funzione discriminante. Infatti, riscrivendo $P(Y=g|X)$ come $\frac{f_g(\boldsymbol x)\pi_g}{\sum_{l=1}^{G}f_l(\boldsymbol x)\pi_l}$ si deduce che per stimare la probabilità di appartenenza ad una classe basta conoscere la funzione di densità della classe stessa, $f_g(\boldsymbol x)$ (e la sua probabilità a priori, $\pi_g$). Partendo da questa formulazione del problema, l'analisi discriminante lineare consiste nell'assumere che le $f_g(\boldsymbol x)$ siano gaussiane $p$-dimensionali con matrice di covarianza comune per tutte le classi. 

Una volta stimati i parametri delle $f_g(x)$ dai dati campionari, è possibile dimostrare che la funzione discriminante associata ad ogni classe risulta essere: 
$$\delta_g(\boldsymbol x)=\boldsymbol x^{T}\Sigma^{-1}\boldsymbol \mu_g -\frac{1}{2}\boldsymbol \mu_g^T\Sigma^{-1}\boldsymbol \mu_g+\log{\pi_g}$$
Per classificare basta poi scegliere $g_*=argmax_g\delta_g(x)$

Passando all'applicazione, il modello presenta una accuratezza pari a 0.7438 e una AUC pari a 0.8095.


```{r, fig.width=9.5, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/lda_line_1_vs_0.Rdata"
load( file )
plot
```

<br></br>
<br></br>


Alternativamente, il modello LDA può essere ricavato come combinazione lineare dei regressori che meglio separa le modalità della variabile di output. In altre parole, si cerca $a^Tx$ tale che i gruppi siano internamente il più omogenei possibili e il più possibile distanti fra loro, i.e. varianza tra le classi ampia e varianza entro piccola. In formule: $$\phi_*=argmax_{\textbf{a}}\frac{\textbf{a}^TB\textbf{a}}{\textbf{a}^TW\textbf{a}}$$
dove $B$ è la matrice di covarianza tra i gruppi e $W$ quella entro.

Risolvendo questo problema di ottimizzazione si ricava:
$$(W^{-1}B-\phi I_{G \times G})\textbf{a}=0$$

Questo sistema di equazioni lineari omogenee ammette soluzioni non banali se e solo se $(W^{-1}B-\phi I)=0$, i.e. se $\phi$ è un autovalore di $W^{-1}B$ e $\boldsymbol a$ contiene il relativo autovettore. Algebricamente, questo implica che ci sono al massimo $G-1$ soluzioni (rango di $W^{-1}B$) e, di conseguenza, $G-1$ direzioni discriminanti. Ora, siccome $\phi$ è proprio la quantità che vogliamo massimizzare, la funzione maggiormente discriminante sarà data dalla combinazione lineare delle features con coefficienti pari dall'autovettore corrispondente all'autovalore maggiore di $W^{-1}B$ e corrisponderà alla superficie che meglio separa un gruppo da tutti gli altri.

Nel caso di 2 classi avremo un solo autovalore non nullo e, quindi, una sola direzione discriminante. Per capire meglio il risultato di questo approccio possiamo infine rappresentare le osservazioni rispetto alla combinazione lineare $\boldsymbol a^Tx$, anche detta coordinata canonica, e sovraimporre il confine di decisione lineare.

```{r, fig.width=9.5, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/canonical_variable.Rdata"
load( file )

file = "results/MODELING/CLASSIFICATION/canonical_variable2.Rdata"
load( file )

subplot( canonical_variable, canonical_variable2) %>% layout(  title = 'Variabile Canonica')

```

<br></br>
<br></br>

Come si nota, le due classi non risultano molto separate. Probabilmente le discrete performance del modello sono dovute al fatto che le 2 categorie siano sbilanciate e che questa funzione discriminante permetta di classificare correttamente la maggior parte delle osservazioni di classe 1 in eccesso.

Si noti che, comparando i risultati di questo secondo approccio e della funzione *lda*, otterremmo a primo acchito un vettore $a$ differente dal *Coefficients of linear discriminants* riportato dalla funzione. Queste differenze sono dovute tuttavia ad una mera questione di parametrizzazione, dal momento che in R viene effettuata una normalizzazione tale da rendere la matrice di covarianza entro i gruppi sferica, i.e.:

$$ \textbf{a}^TW\textbf{a} = \Psi_{diag} $$

Pertanto, partendo dal secondo metodo è possibile ottenere gli stessi coefficienti della funzione *lda* post-moltiplicando il vettore $\textbf{a}$ per la matrice $\Psi^{-\frac{1}{2}}$.

<br></br>
<br></br>