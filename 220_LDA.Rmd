##<a name="22_LDA" style="pointer-events: none; cursor: default;" ><font color="red"> 2.2. Linear Discriminant Analysis</font></a>

L'analisi discriminante lineare (LDA) consiste in un tentativo alternativo di costruire una funzione discriminante. Infatti, riscrivendo $P(Y=g|X)$ come $\frac{f_g(x)\pi_g}{\sum_{l=1}^{G}f_l(x)\pi_l}$ si deduce che per stimare la probabilità di appartenenza ad una classe basta conoscere la funzione di densità della classe stessa (e la sua probabilità a priori). Partendo da questa formulazione del problema, l'analisi discriminante lineare consiste nell'assumere che le $f_g(x)$ siano gaussiane $p+1$-dimensionali con matrice di covarianza comune per tutte le classi. 

Una volta stimate le $f_g(x)$ dai dati campionari, è possibile dimostrare che la funzione discriminante associata ad ogni classe risulta essere: 
$$\delta_g(x)=x^{T}\Sigma^{-1}\mu_g -\frac{1}{2}\mu_g^T\Sigma^{-1}\mu_g+\log{\pi_g}$$
Per classificare basta poi scegliere $g_*=argmax_g\delta_g(x)$

**Risultati e plot distribuzione nella direzione discriminante**

```{r, fig.width=10, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/canonical_variable.Rdata"
load( file )

file = "results/MODELING/CLASSIFICATION/canonical_variable2.Rdata"
load( file )

subplot( canonical_variable, canonical_variable2) %>% layout(  title = 'Variabile Canonica')

```



Alternativamente, il modello LDA può essere ricavato come combinazione lineare dei regressori che meglio separa le modalità della variabile di output. In altre parole, si cerca $a^Tx$ tale che i gruppi siano internamente il più omogenei possibili e il più possibile distanti fra loro, i.e. varianza tra le classi ampia e varianza entro piccola. In formule: $$\phi_*=argmax_a\frac{a^TBa}{a^tWa}$$
dove $B$ è la matrice di covarianza tra i gruppi e $W$ quella entro.

Risolvendo questo problema di ottimizzazione si ricava:
$$(W^{-1}B-\phi I_{G x G})a=0$$

Questo sistema di equazioni lineari omogenee ammette soluzioni non banali se e solo se $(W^{-1}B-\phi I)=0$, i.e. se $\phi$ è un autovalore di $W^{-1}B$ e $a$ contiene il relativo autovettore. Algebricamente, questo implica che ci sono al massimo $G-1$ soluzioni (rango di $W^{-1}B$) e, di conseguenza, $G-1$ direzioni discriminanti. Ora, siccome $\phi$ è proprio la quantità che vogliamo massimizzare, la funzione maggiormente discriminante sarà data dall'autovettore corrispondente all'autovalore maggiore di $W^{-1}B$ e corrisponderà alla superficie che meglio separa un gruppo da tutti gli altri.

Nel caso di 2 classi avremo un solo autovalore non nullo e, quindi, una sola direzione discriminante. Per capire meglio il risultato di questo approccio possiamo infine rappresentare le osservazioni rispetto alla combinazione lineare $a^Tx$, anche detta coordinata canonica, e sovraimporre il confine di decisione lineare.

```{r, fig.width=10, fig.height=4, echo=FALSE}
file = "results/MODELING/CLASSIFICATION/lda_line_1_vs_0.Rdata"
load( file )
plot
```
Come si nota, le due classi non risultano molto separate. Probabilmente le discrete performance del modello sono dovute al fatto che le 2 categorie siano sbilanciate e che questa funzione discriminante permetta di classificare correttamente la maggior parte delle osservazioni di classe 1 in eccesso.

**verifica che le due formulazioni siano uguali**
